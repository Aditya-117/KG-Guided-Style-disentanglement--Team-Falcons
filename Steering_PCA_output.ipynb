{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "login-cell"
      },
      "source": [
        "## **Step 0**: Log in to Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "login-code"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a8ce73023fe4b269094b80ba56230e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "try:\n",
        "    from huggingface_hub import notebook_login\n",
        "    notebook_login()\n",
        "except ImportError:\n",
        "    print(\"huggingface_hub not found. Please log in using 'huggingface-cli login' in your terminal.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install-cell"
      },
      "source": [
        "## **Step 1**: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "install-code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers torch scikit-learn accelerate tqdm pandas openpyxl numpy -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "helper-cell"
      },
      "source": [
        "## **Step 2**: Imports and Helper Classes\n",
        "\n",
        "This cell defines all the necessary functions and classes: the model wrapper (for generation), the steering hook, and the vector calculation functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "helper-code"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import argparse\n",
        "\n",
        "# --- Lightweight Model Wrapper (for Generation) ---\n",
        "class ModelSteeringWrapper:\n",
        "    def __init__(self, model_name: str):\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        self.device = self.model.device\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        self._layers_attr_path = self._find_layer_attr_path()\n",
        "        self.num_layers = len(self._get_layers_list())\n",
        "        print(f\"[ModelSteeringWrapper] Model loaded. Path: {self._layers_attr_path}, Layers: {self.num_layers}\")\n",
        "\n",
        "    def _find_layer_attr_path(self):\n",
        "        candidates = [[\"model\", \"layers\"], [\"transformer\", \"h\"], [\"model\", \"decoder\", \"layers\"]]\n",
        "        for path in candidates:\n",
        "            cur = self.model\n",
        "            valid = True\n",
        "            for p in path:\n",
        "                if hasattr(cur, p): cur = getattr(cur, p)\n",
        "                else: valid = False; break\n",
        "            if valid and isinstance(cur, (list, nn.ModuleList)): return path\n",
        "        raise AttributeError(\"Could not find transformer layer list in model.\")\n",
        "\n",
        "    def _get_layers_list(self):\n",
        "        cur = self.model\n",
        "        for p in self._layers_attr_path: cur = getattr(cur, p)\n",
        "        return list(cur)\n",
        "\n",
        "    def generate(self, prompt: str, max_new_tokens: int = 150, **kwargs) -> str:\n",
        "        tok = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "        input_token_len = tok.input_ids.shape[1]\n",
        "        out = self.model.generate(**tok, max_new_tokens=max_new_tokens, pad_token_id=self.tokenizer.pad_token_id, **kwargs)\n",
        "        full_tokens = out[0]\n",
        "        new_tokens = full_tokens[input_token_len:]\n",
        "        generated_text = self.tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "        return generated_text.strip()\n",
        "\n",
        "# --- Style Vector Extraction Methods ---\n",
        "def compute_mean_difference(pos: np.ndarray, neg: np.ndarray) -> np.ndarray:\n",
        "    diff = (pos - neg).mean(axis=0)\n",
        "    return diff / (np.linalg.norm(diff) + 1e-12)\n",
        "\n",
        "def compute_logistic_regression(pos: np.ndarray, neg: np.ndarray) -> np.ndarray:\n",
        "    X = np.vstack([pos, neg])\n",
        "    y = np.concatenate([np.ones(len(pos)), np.zeros(len(neg))])\n",
        "    clf = LogisticRegression(max_iter=1000).fit(X, y)\n",
        "    w = clf.coef_.reshape(-1)\n",
        "    return w / (np.linalg.norm(w) + 1e-12)\n",
        "\n",
        "def compute_pca_vector(pos: np.ndarray, neg: np.ndarray) -> np.ndarray:\n",
        "    diffs = pos - neg\n",
        "    pca = PCA(n_components=1).fit(np.vstack([diffs, -diffs]))\n",
        "    vec = pca.components_[0]\n",
        "    return vec / (np.linalg.norm(vec) + 1e-12)\n",
        "\n",
        "# --- Steering Hook Class ---\n",
        "class SteeringHook:\n",
        "    def __init__(self, model, layer_path, layer_idx, style_vector, multiplier):\n",
        "        self.model, self.layer_path, self.layer_idx = model, layer_path, layer_idx\n",
        "        self.style_vector_cpu = torch.from_numpy(style_vector).float() * multiplier\n",
        "        self.handle = None\n",
        "        self._register_hook()\n",
        "\n",
        "    def _get_layer_module(self):\n",
        "        cur = self.model\n",
        "        for p in self.layer_path: cur = getattr(cur, p)\n",
        "        idx = self.layer_idx if self.layer_idx >= 0 else len(cur) + self.layer_idx\n",
        "        return cur[idx]\n",
        "\n",
        "    def _hook(self, module, input, output):\n",
        "        tensor_output = output[0] if isinstance(output, tuple) else output\n",
        "        add_vec = self.style_vector_cpu.to(tensor_output.device, dtype=tensor_output.dtype)\n",
        "        modified_tensor = tensor_output + add_vec.view(1, 1, -1)\n",
        "        return (modified_tensor,) + output[1:] if isinstance(output, tuple) else modified_tensor\n",
        "\n",
        "    def _register_hook(self):\n",
        "        self.handle = self._get_layer_module().register_forward_hook(self._hook)\n",
        "\n",
        "    def remove(self):\n",
        "        if self.handle: self.handle.remove()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "main-cell"
      },
      "source": [
        "## **Step 3**: Load Activations, Compute Vectors, and Run Test\n",
        "\n",
        "This is the main driver cell. It loads the saved activations, calculates the three vectors, and generates a steered response for the *first email* in the spreadsheet using each method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "main-code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running inference: model=meta-llama/Llama-2-7b-hf, layer=-15\n",
            "Successfully loaded activations from 'activations.npz'\n",
            "Computing style vectors...\n",
            "All style vectors computed.\n",
            "Loading Llama 2 model... (This may take a few minutes)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81829c431d2c4b93bacb7984d4fbfbb5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ModelSteeringWrapper] Model loaded. Path: ['model', 'layers'], Layers: 32\n",
            "\n",
            "==================================================\n",
            "Steering Demonstration for First Email\n",
            "==================================================\n",
            "Test Query:\n",
            "Draft an email invitation for the Live SouJava- Microservice Patterns - Implemented by Eclipse Microprofile, scheduled for 2020-04-20 00:00:00, at 07:00:00 in the San-Francisco. The event is hosted and sent by Product School Bangalore.\n",
            "\n",
            "Subject: You're Invited! Live SouJava: Microservice Patterns in San-Francisco!\n",
            "\n",
            "\n",
            "\n",
            "--- Ideal Styled Response (from file) ---\n",
            "Subject: You're Invited! Live SouJava: Microservice Patterns in San-Francisco!\n",
            "\n",
            "Hi there,\n",
            "\n",
            "We'd be delighted if you could join us for a special Product School Bangalore event!\n",
            "\n",
            "Live SouJava: Microservice Patterns - Implemented by Eclipse Microprofile\n",
            "\n",
            "It's happening on April 20th at 7:00 AM in San-Francisco. We think you'll find this blend of technology and business insights super valuable.\n",
            "\n",
            "Hope to see you there!\n",
            "\n",
            "Warmly,\n",
            "\n",
            "The Product School Bangalore Team\n",
            "\n",
            "--- Steered Generated Response (Live, Method: MEAN) ---\n",
            "Hi {first.last},\n",
            "\n",
            "I'd like to invite you to the Live SouJava: Microservice Patterns in San-Francisco!\n",
            "\n",
            "### Who is this event for?\n",
            "\n",
            "This event is for:\n",
            "\n",
            "- Software developers who want to learn and discuss Microservice Patterns\n",
            "\n",
            "### What will you learn?\n",
            "\n",
            "- What are microservice patterns and why they are important?\n",
            "- How to implement microservice patterns using Eclipse Microprofile\n",
            "\n",
            "### What to bring?\n",
            "\n",
            "- Laptop (to follow the tutorial)\n",
            "- Charger\n",
            "\n",
            "### What do I need to prepare before the event?\n",
            "\n",
            "- We recommend that you take a look at the documentation\n",
            "\n",
            "--- Steered Generated Response (Live, Method: LOGREG) ---\n",
            "Date: 2020-04-20 00:00:00\n",
            "\n",
            "Location: San-Francisco\n",
            "\n",
            "You are invited to attend Live SouJava: Microservice Patterns in San-Francisco on April 20, 2020!\n",
            "\n",
            "Speaker: SouJava\n",
            "\n",
            "You can RSVP here: https://www.meetup.com/Product-School-Bangalore/events/269083670/\n",
            "\n",
            "We look forward to seeing you there!\n",
            "\n",
            "Team Product School\n",
            "\n",
            "--- Steered Generated Response (Live, Method: PCA) ---\n",
            "Hi,\n",
            "\n",
            "I'd like to invite you to join me for a live session on \"Microservice Patterns\".\n",
            "\n",
            "Date: April 20, 2020\n",
            "\n",
            "Time: 7:00pm - 9:00pm\n",
            "\n",
            "Location: San-Francisco\n",
            "\n",
            "This is a live online event. You can join from anywhere in the world.\n",
            "\n",
            "Please register here: https://www.productschool.com/live-soujava/microservice-patterns\n",
            "\n",
            "If you have any questions, please let me know.\n",
            "\n",
            "Thank you!\n",
            "\n",
            "SouJava\n",
            "\n",
            "SouJava is a meetup group of product professionals,\n",
            "\n",
            "Test finished.\n"
          ]
        }
      ],
      "source": [
        "def run_inference_test(model_name: str, layer_index: int, xlsx_path: str, activations_path: str):\n",
        "    \n",
        "    # --- 1. Load Activations and Compute Vectors ---\n",
        "    try:\n",
        "        data = np.load(activations_path)\n",
        "        pos_arr = data['pos_acts']\n",
        "        neg_arr = data['neg_acts']\n",
        "        print(f\"Successfully loaded activations from '{activations_path}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading '{activations_path}'. Please run the activation extraction script first.\")\n",
        "        print(f\"Error details: {e}\")\n",
        "        return\n",
        "\n",
        "    print(\"Computing style vectors...\")\n",
        "    style_vectors = {\n",
        "        \"mean\": compute_mean_difference(pos_arr, neg_arr),\n",
        "        \"logreg\": compute_logistic_regression(pos_arr, neg_arr),\n",
        "        \"pca\": compute_pca_vector(pos_arr, neg_arr)\n",
        "    }\n",
        "    print(\"All style vectors computed.\")\n",
        "\n",
        "    # --- 2. Load the First Row from Excel for the Test --- \n",
        "    try:\n",
        "        df = pd.read_excel(xlsx_path, nrows=2)\n",
        "        test_row = df.iloc[1]\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading Excel file '{xlsx_path}': {e}\")\n",
        "        return\n",
        "\n",
        "   # --- Check for correct columns (case-sensitive) ---\n",
        "    NEUTRAL_COL = 'response_Neutral' # Or 'resonse_neutral'?\n",
        "    STYLED_COL = 'response_styled'\n",
        "    FACT_COLS = ['date', 'Time', 'Venue', 'OccasionType', 'Host', 'Event']\n",
        "    required_cols = FACT_COLS + [NEUTRAL_COL, STYLED_COL]\n",
        "    \n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        print(\"Error: Missing one of the required columns in your Excel file.\")\n",
        "        print(f\"Found: {df.columns.to_list()}\")\n",
        "        return\n",
        "\n",
        "    # --- Dynamically build the prompt from the first row ---\n",
        "    test_row = df.iloc[1]\n",
        "\n",
        "    # Get facts from the row\n",
        "    event = str(test_row.get('Event', 'N/A'))\n",
        "    date = str(test_row.get('date', 'N/A'))\n",
        "    time_ = str(test_row.get('Time', 'N/A'))\n",
        "    venue = str(test_row.get('Venue', 'N/A'))\n",
        "    host = str(test_row.get('Host', 'N/A'))\n",
        "\n",
        "    # 1. Build the test_query in the exact format requested\n",
        "    test_query = f\"Draft an email invitation for the {event}, scheduled for {date}, at {time_} in the {venue}. The event is hosted and sent by {host}.\"\n",
        "\n",
        "    # 2. Get the ideal response from the file\n",
        "    ideal_response = str(test_row.get(STYLED_COL))\n",
        "\n",
        "    # 3. Extract the subject line from the ideal response to build the final prompt\n",
        "    try:\n",
        "        subject_line = ideal_response.split('\\n')[0]\n",
        "    except Exception:\n",
        "        subject_line = \"Subject: Invitation\" # Fallback\n",
        "\n",
        "    prompt = f\"{test_query}\\n\\n{subject_line}\\n\\n\"\n",
        "\n",
        "    # --- 3. Load Model --- \n",
        "    print(\"Loading Llama 2 model... (This may take a few minutes)\")\n",
        "    ae = ModelSteeringWrapper(model_name)\n",
        "\n",
        "    # --- 4. Run Steering Demonstration --- \n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Steering Demonstration for First Email\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Test Query:\\n{prompt}\")\n",
        "    print(\"\\n--- Ideal Styled Response (from file) ---\")\n",
        "    print(ideal_response)\n",
        "\n",
        "    # Use the multiplier you found was best (e.g., 3.0 or 0.5)\n",
        "    # Or just a standard one to compare them (e.g., 2.0)\n",
        "    MULTIPLIER = 3.0 \n",
        "\n",
        "    for method, style_vec in style_vectors.items():\n",
        "        print(f\"\\n--- Steered Generated Response (Live, Method: {method.upper()}) ---\")\n",
        "        \n",
        "        hook = SteeringHook(ae.model, ae._layers_attr_path, layer_index, style_vec, MULTIPLIER)\n",
        "        try:\n",
        "            steered_out = ae.generate(prompt, temperature=0.7, do_sample=True, top_p=0.9)\n",
        "            print(steered_out)\n",
        "        finally:\n",
        "            hook.remove()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # This allows running from a notebook cell\n",
        "    if 'ipykernel' in sys.modules: sys.argv = sys.argv[:1]\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model\", type=str, default=\"meta-llama/Llama-2-7b-hf\")\n",
        "    parser.add_argument(\"--layer\", type=int, default=-15)\n",
        "    parser.add_argument(\"--xlsx_file\", type=str, default=\"generated_email_responses (1).xlsx\")\n",
        "    parser.add_argument(\"--activations_file\", type=str, default=\"activations.npz\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(f\"Running inference: model={args.model}, layer={args.layer}\")\n",
        "    run_inference_test(args.model, args.layer, args.xlsx_file, args.activations_file)\n",
        "    print(\"\\nTest finished.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
