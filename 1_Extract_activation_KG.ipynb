{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md-title"
      },
      "source": [
        "## **Part 1: Extract Activations from Pre-Modified Text**\n",
        "\n",
        "This script loads the Excel file, which *already* contains defactualized (placeholder) responses. It reads these modified columns, extracts activations from them, and saves the `activations.npz` file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install-cell"
      },
      "source": [
        "### **Step 0**: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "install-code"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch scikit-learn accelerate tqdm pandas openpyxl numpy -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "login-cell"
      },
      "source": [
        "### **Step 1**: Log in to Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "login-code"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb296614d43e4a709640929d3a7ed882",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "try:\n",
        "    from huggingface_hub import notebook_login\n",
        "    notebook_login()\n",
        "except ImportError:\n",
        "    print(\"huggingface_hub not found. Please log in using 'huggingface-cli login' in your terminal.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model-wrapper-cell"
      },
      "source": [
        "### **Step 2**: Activation Extractor Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "model-wrapper-code"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import sys\n",
        "import argparse\n",
        "import re\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "class ActivationExtractor:\n",
        "    def __init__(self, model_name: str):\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        self.device = self.model.device\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        self._layers_attr_path = self._find_layer_attr_path()\n",
        "        self.num_layers = len(self._get_layers_list())\n",
        "        print(f\"[ActivationExtractor] Model loaded. Path: {self._layers_attr_path}, Layers: {self.num_layers}\")\n",
        "\n",
        "    def _find_layer_attr_path(self):\n",
        "        candidates = [[\"model\", \"layers\"], [\"transformer\", \"h\"], [\"model\", \"decoder\", \"layers\"]]\n",
        "        for path in candidates:\n",
        "            cur = self.model\n",
        "            valid = True\n",
        "            for p in path:\n",
        "                if hasattr(cur, p): cur = getattr(cur, p)\n",
        "                else: valid = False; break\n",
        "            if valid and isinstance(cur, (list, nn.ModuleList)): return path\n",
        "        raise AttributeError(\"Could not find transformer layer list in model.\")\n",
        "\n",
        "    def _get_layers_list(self):\n",
        "        cur = self.model\n",
        "        for p in self._layers_attr_path: cur = getattr(cur, p)\n",
        "        return list(cur)\n",
        "\n",
        "    def _resolve_layer_idx(self, idx: int):\n",
        "        L = self.num_layers\n",
        "        if idx < 0: idx = L + idx\n",
        "        assert 0 <= idx < L, f\"layer_index {idx} out of range\"\n",
        "        return idx\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def get_activation_for_pair(self, input_text: str, output_text: str, layer_index: int) -> np.ndarray:\n",
        "        idx = self._resolve_layer_idx(layer_index)\n",
        "        concat = f\"{input_text.strip()} {output_text.strip()}\"\n",
        "        tok = self.tokenizer(concat, return_tensors=\"pt\").to(self.model.device)\n",
        "        outputs = self.model(**tok, output_hidden_states=True, return_dict=True)\n",
        "        hs = outputs.hidden_states[idx + 1]\n",
        "        return hs[0, -1, :].detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "main-driver-cell"
      },
      "source": [
        "## **Step 3**: Main Extraction Driver\n",
        "\n",
        "This is the main part of the script. It will:\n",
        "1.  Load the Excel file.\n",
        "2.  Read the defactualized text from `response_Neutral_Modified` and `response_Modified`.\n",
        "3.  Create a placeholder prompt for each row.\n",
        "4.  Run activation extraction and save the activations by the name `activations.npz`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "main-driver-code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Activation Extraction: model=meta-llama/Llama-2-7b-hf, layer=-15\n",
            "INFO: Loaded 41 examples from the file.\n",
            "Successfully created 41 training pairs.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2854cf6b00543fabe764122119b3127",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ActivationExtractor] Model loaded. Path: ['model', 'layers'], Layers: 32\n",
            "\n",
            "[Pipeline] Extracting activations for 'user_1' with 41 examples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting training activations: 100%|██████████| 41/41 [00:59<00:00,  1.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[SUCCESS] Activations saved successfully to 'activations.npz'\n",
            "  Positive activations shape: (41, 4096)\n",
            "  Negative activations shape: (41, 4096)\n",
            "You can now run the next notebook for activation steering.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def load_data_for_extraction(file_path: str) -> Optional[Dict[str, List[Tuple[str, str, str]]]]:\n",
        "    \"\"\"\n",
        "    Loads pre-defactualized data from the XLSX file and creates prompts.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_excel(file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while reading the Excel file: {e}\")\n",
        "        return None\n",
        "\n",
        "    # --- Column Names --- \n",
        "    \n",
        "    DEFACT_NEUTRAL_COL = 'response_Neutral'\n",
        "    DEFACT_STYLED_COL = 'response_Modified'\n",
        "    FACT_COLS_PLACEHOLDERS = ['extracted_DATE', 'extracted_TIME', 'extracted_VENUE', 'extracted_HOST', 'extracted_EVENT']\n",
        "    # -----------------------------------\n",
        "    \n",
        "    required_cols = FACT_COLS_PLACEHOLDERS + [DEFACT_NEUTRAL_COL, DEFACT_STYLED_COL]\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        print(\"Error: Missing one of the required columns.\")\n",
        "        print(f\"Script needs: {required_cols}\")\n",
        "        print(f\"Found in file: {df.columns.to_list()}\")\n",
        "        # Try to continue if at least the core columns are there\n",
        "        if DEFACT_NEUTRAL_COL not in df.columns or DEFACT_STYLED_COL not in df.columns:\n",
        "            return None \n",
        "        print(\"Warning: Missing some fact columns, prompts may be incomplete.\")\n",
        "\n",
        "    print(f\"INFO: Loaded {len(df)} examples from the file.\")\n",
        "    train_examples = []\n",
        "    \n",
        "    for idx, row in df.iterrows():\n",
        "        # 1. Get facts (as placeholders, e.g., \"<DATE>\") from the row\n",
        "        date = str(row.get('extracted_DATE', '<DATE>'))\n",
        "        time_ = str(row.get('extracted_TIME', '<TIME>'))\n",
        "        venue = str(row.get('extracted_VENUE', '<VENUE>'))\n",
        "        host = str(row.get('extracted_HOST', '<HOST>'))\n",
        "        event = str(row.get('extracted_EVENT', '<EVENT>'))\n",
        "        \n",
        "        # 2. Get the *already defactualized* email text\n",
        "        neutral_email = str(row.get(DEFACT_NEUTRAL_COL))\n",
        "        styled_email = str(row.get(DEFACT_STYLED_COL))\n",
        "\n",
        "        if pd.isna(neutral_email) or pd.isna(styled_email) or neutral_email == 'nan' or styled_email == 'nan':\n",
        "            print(f\"Skipping row {idx} due to missing email text.\")\n",
        "            continue\n",
        "\n",
        "        # 3. Extract subject from the defactualized neutral email\n",
        "        subject_line = \"Subject: Invitation\" # Default fallback\n",
        "        match = re.search(r'Subject:\\\\s*(.*)', neutral_email, re.IGNORECASE)\n",
        "        if match:\n",
        "            subject_line = match.group(0).strip() # Get the full \"Subject: ...\" line\n",
        "\n",
        "        # 4. Create the dynamic prompt using the placeholders\n",
        "        prompt = f\"\"\"Write an email invitation for the {event}, scheduled for {date}, at {time_} in the {venue}. The event is hosted and sent by {host}.\n",
        "        \n",
        "{subject_line}\n",
        "\"\"\"\n",
        "        \n",
        "        # 5. Add to training set\n",
        "        train_examples.append((prompt, styled_email, neutral_email))\n",
        "\n",
        "    print(f\"Successfully created {len(train_examples)} training pairs.\")\n",
        "    return {\"user_1\": train_examples}\n",
        "\n",
        "# --- Main Execution Block ---\n",
        "if __name__ == \"__main__\":\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        sys.argv = sys.argv[:1]\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model\", type=str, default=\"meta-llama/Llama-2-7b-hf\")\n",
        "    parser.add_argument(\"--layer\", type=int, default=-15)\n",
        "    parser.add_argument(\"--xlsx_file\", type=str, default=\"generated_email_responses_modified (2).xlsx\")\n",
        "    parser.add_argument(\"--output_file\", type=str, default=\"activations.npz\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(f\"Running Activation Extraction: model={args.model}, layer={args.layer}\")\n",
        "    \n",
        "    # 1. Load data\n",
        "    train_hist = load_data_for_extraction(args.xlsx_file)\n",
        "    if not train_hist:\n",
        "        print(\"Halting execution due to data loading error.\")\n",
        "    else:\n",
        "        # 2. Load model\n",
        "        ae = ActivationExtractor(args.model)\n",
        "\n",
        "        # 3. Extract Activations\n",
        "        user_id = \"user_1\"\n",
        "        examples = train_hist[user_id]\n",
        "        \n",
        "        print(f\"\\n[Pipeline] Extracting activations for '{user_id}' with {len(examples)} examples...\")\n",
        "        pos_acts, neg_acts = [], []\n",
        "        \n",
        "        for (inp_prompt, user_out, neutral_out) in tqdm(examples, desc=\"Extracting training activations\"):\n",
        "            pos_acts.append(ae.get_activation_for_pair(inp_prompt, user_out, args.layer))\n",
        "            neg_acts.append(ae.get_activation_for_pair(inp_prompt, neutral_out, args.layer))\n",
        "\n",
        "        # 4. Save the activations to a file\n",
        "        if not pos_acts:\n",
        "            print(\"\\nNo activations were extracted. Please check your data file.\")\n",
        "        else:\n",
        "            pos_arr = np.vstack(pos_acts)\n",
        "            neg_arr = np.vstack(neg_acts)\n",
        "            \n",
        "            np.savez_compressed(args.output_file, pos_acts=pos_arr, neg_acts=neg_arr)\n",
        "            \n",
        "            print(f\"\\n[SUCCESS] Activations saved successfully to '{args.output_file}'\")\n",
        "            print(f\"  Positive activations shape: {pos_arr.shape}\")\n",
        "            print(f\"  Negative activations shape: {neg_arr.shape}\")\n",
        "            print(\"You can now run the next notebook for activation steering.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
