{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install-cell"
      },
      "source": [
        "## **Step 0**: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "install-code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers torch scikit-learn accelerate tqdm pandas openpyxl numpy rouge-score nltk -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "login-cell"
      },
      "source": [
        "## **Step 1**: Log in to Hugging Face\n",
        "\n",
        "Run this cell once. If you are running locally and have already used `huggingface-cli login` in your terminal, you can skip this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "login-code"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "66efc37421ed4082821d42bb8ca57562",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "try:\n",
        "    from huggingface_hub import notebook_login\n",
        "    notebook_login()\n",
        "except ImportError:\n",
        "    print(\"huggingface_hub not found. Please log in using 'huggingface-cli login' in your terminal.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "helper-classes-cell"
      },
      "source": [
        "## **Step 2**: Helper Classes & Functions\n",
        "\n",
        "This cell contains all helper classes:\n",
        "1.  **`ModelSteeringWrapper`**: For generation.\n",
        "2.  **`PlaceholderReplacer`**: Your code for re-hydrating text.\n",
        "3.  **`SteeringHook`**: For applying vectors.\n",
        "4.  **`compute_...` functions**: For building vectors from loaded data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "helper-code"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to C:\\Users\\CSE IIT\n",
            "[nltk_data]     BHILAI\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import argparse\n",
        "import re\n",
        "import json\n",
        "import ast\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Tuple\n",
        "# Add these new imports\n",
        "from rouge_score import rouge_scorer\n",
        "import nltk\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "\n",
        "# Download NLTK data needed for METEOR\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "# --- 1. Lightweight Model Wrapper (for Generation) ---\n",
        "class ModelSteeringWrapper:\n",
        "    def __init__(self, model_name: str):\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        self.device = self.model.device\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self._layers_attr_path = self._find_layer_attr_path()\n",
        "        self.num_layers = len(self._get_layers_list())\n",
        "        print(f\"[ModelSteeringWrapper] Model loaded. Path: {self._layers_attr_path}, Layers: {self.num_layers}\")\n",
        "\n",
        "    def _find_layer_attr_path(self):\n",
        "        candidates = [[\"model\", \"layers\"], [\"transformer\", \"h\"], [\"model\", \"decoder\", \"layers\"]]\n",
        "        for path in candidates:\n",
        "            cur = self.model\n",
        "            valid = True\n",
        "            for p in path:\n",
        "                if hasattr(cur, p): cur = getattr(cur, p)\n",
        "                else: valid = False; break\n",
        "            if valid and isinstance(cur, (list, nn.ModuleList)): return path\n",
        "        raise AttributeError(\"Could not find transformer layer list in model.\")\n",
        "\n",
        "    def _get_layers_list(self):\n",
        "        cur = self.model\n",
        "        for p in self._layers_attr_path: cur = getattr(cur, p)\n",
        "        return list(cur)\n",
        "\n",
        "    def generate(self, prompt: str, max_new_tokens: int = 150, **kwargs) -> str:\n",
        "        tok = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "        input_token_len = tok.input_ids.shape[1]\n",
        "        out = self.model.generate(**tok, max_new_tokens=max_new_tokens, pad_token_id=self.tokenizer.pad_token_id, **kwargs)\n",
        "        full_tokens = out[0]\n",
        "        new_tokens = full_tokens[input_token_len:]\n",
        "        generated_text = self.tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "        return generated_text.strip()\n",
        "\n",
        "# --- 2. Your PlaceholderReplacer Class (for Re-hydration) ---\n",
        "class PlaceholderReplacer:\n",
        "    \"\"\"Replace placeholders with actual entity values from extracted columns\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.entity_types = ['EVENT', 'DATE', 'TIME', 'VENUE', 'HOST']\n",
        "    \n",
        "    def parse_entity_list(self, entity_str):\n",
        "        \"\"\"Parse string representation of list back to actual list\"\"\"\n",
        "        if pd.isna(entity_str) or entity_str == '[]' or entity_str == '':\n",
        "            return []\n",
        "        \n",
        "        try:\n",
        "            # Try to evaluate as Python literal\n",
        "            return ast.literal_eval(entity_str)\n",
        "        except:\n",
        "            # If that fails, return empty list\n",
        "            return []\n",
        "            \n",
        "    def build_entity_dict_from_row(self, row, fact_cols):\n",
        "        \"\"\"Helper to create the entity dict from a DataFrame row\"\"\"\n",
        "        entities_dict = {}\n",
        "        for entity_type in self.entity_types:\n",
        "            column_name = f'extracted_{entity_type}'\n",
        "            if column_name in fact_cols and column_name in row:\n",
        "                entity_str = row[column_name]\n",
        "                entities_dict[entity_type] = self.parse_entity_list(entity_str)\n",
        "        return entities_dict\n",
        "    \n",
        "    def replace_placeholders(self, text, entities_dict):\n",
        "        \"\"\"Replace all placeholders in text with actual entity values\"\"\"\n",
        "        \n",
        "        if not text or pd.isna(text):\n",
        "            return text, {}\n",
        "        \n",
        "        replaced_text = str(text)\n",
        "        replacement_log = {}\n",
        "        \n",
        "        # Sort entities by length of first fact (longest first) to avoid partial matches\n",
        "        sorted_entity_types = sorted(\n",
        "            self.entity_types,\n",
        "            key=lambda et: len(str(entities_dict.get(et, [''])[0])) if entities_dict.get(et) else 0,\n",
        "            reverse=True\n",
        "        )\n",
        "\n",
        "        for entity_type in sorted_entity_types:\n",
        "            entity_list = entities_dict.get(entity_type, [])\n",
        "            \n",
        "            if not entity_list:\n",
        "                continue\n",
        "            \n",
        "            placeholder = f'<{entity_type}>'\n",
        "            # Use regex for case-insensitive placeholder matching\n",
        "            placeholder_pattern = re.compile(re.escape(placeholder), re.IGNORECASE)\n",
        "            \n",
        "            # Find all matches\n",
        "            matches = list(placeholder_pattern.finditer(replaced_text))\n",
        "            placeholder_count = len(matches)\n",
        "            \n",
        "            if placeholder_count == 0:\n",
        "                continue\n",
        "            \n",
        "            replacements_made = []\n",
        "            # We reverse the matches to replace from the end first to not mess up indices\n",
        "            for i, match in enumerate(reversed(matches)):\n",
        "                # Find which entity to use\n",
        "                entity_idx = i % len(entity_list)\n",
        "                replacement_value = str(entity_list[entity_idx])\n",
        "                \n",
        "                # Replace this specific match\n",
        "                start, end = match.span()\n",
        "                replaced_text = replaced_text[:start] + replacement_value + replaced_text[end:]\n",
        "                replacements_made.append(f\"{match.group(0)} → {replacement_value}\")\n",
        "            \n",
        "            replacement_log[entity_type] = list(reversed(replacements_made))\n",
        "            \n",
        "        return replaced_text, replacement_log\n",
        "\n",
        "# --- 3. Style Vector Extraction Methods ---\n",
        "def compute_mean_difference(pos: np.ndarray, neg: np.ndarray) -> np.ndarray:\n",
        "    diff = (pos - neg).mean(axis=0)\n",
        "    return diff / (np.linalg.norm(diff) + 1e-12)\n",
        "\n",
        "def compute_logistic_regression(pos: np.ndarray, neg: np.ndarray) -> np.ndarray:\n",
        "    X = np.vstack([pos, neg])\n",
        "    y = np.concatenate([np.ones(len(pos)), np.zeros(len(neg))])\n",
        "    clf = LogisticRegression(max_iter=1000).fit(X, y)\n",
        "    w = clf.coef_.reshape(-1)\n",
        "    return w / (np.linalg.norm(w) + 1e-12)\n",
        "\n",
        "def compute_pca_vector(pos: np.ndarray, neg: np.ndarray) -> np.ndarray:\n",
        "    diffs = pos - neg\n",
        "    pca = PCA(n_components=1).fit(np.vstack([diffs, -diffs]))\n",
        "    vec = pca.components_[0]\n",
        "    return vec / (np.linalg.norm(vec) + 1e-12)\n",
        "\n",
        "# --- 4. Steering Hook Class ---\n",
        "class SteeringHook:\n",
        "    def __init__(self, model, layer_path, layer_idx, style_vector, multiplier):\n",
        "        self.model, self.layer_path, self.layer_idx = model, layer_path, layer_idx\n",
        "        self.style_vector_cpu = torch.from_numpy(style_vector).float() * multiplier\n",
        "        self.handle = None\n",
        "        self._register_hook()\n",
        "\n",
        "    def _get_layer_module(self):\n",
        "        cur = self.model\n",
        "        for p in self.layer_path: cur = getattr(cur, p)\n",
        "        idx = self.layer_idx if self.layer_idx >= 0 else len(cur) + self.layer_idx\n",
        "        return cur[idx]\n",
        "\n",
        "    def _hook(self, module, input, output):\n",
        "        tensor_output = output[0] if isinstance(output, tuple) else output\n",
        "        add_vec = self.style_vector_cpu.to(tensor_output.device, dtype=tensor_output.dtype)\n",
        "        modified_tensor = tensor_output + add_vec.view(1, 1, -1)\n",
        "        return (modified_tensor,) + output[1:] if isinstance(output, tuple) else modified_tensor\n",
        "\n",
        "    def _register_hook(self):\n",
        "        self.handle = self._get_layer_module().register_forward_hook(self._hook)\n",
        "\n",
        "    def remove(self):\n",
        "        if self.handle: self.handle.remove()\n",
        "    \n",
        "# --- 5. Evaluation Function ---\n",
        "def calculate_scores(references: List[str], candidates: List[str]):\n",
        "    \"\"\"\n",
        "    Calculates average ROUGE (1, 2, L) and METEOR scores\n",
        "    for a list of generated candidates and their references.\n",
        "    \"\"\"\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    \n",
        "    total_rouge1 = 0\n",
        "    total_rouge2 = 0\n",
        "    total_rougeL = 0\n",
        "    total_meteor = 0\n",
        "    \n",
        "    num_samples = len(references)\n",
        "    if num_samples == 0:\n",
        "        return {\n",
        "            \"avg_rouge1\": 0,\n",
        "            \"avg_rouge2\": 0,\n",
        "            \"avg_rougeL\": 0,\n",
        "            \"avg_meteor\": 0\n",
        "        }\n",
        "\n",
        "    for ref, cand in zip(references, candidates):\n",
        "        if not ref or not cand: # Handle empty strings\n",
        "            num_samples -= 1\n",
        "            continue\n",
        "            \n",
        "        # ROUGE\n",
        "        rouge_scores = scorer.score(ref, cand)\n",
        "        total_rouge1 += rouge_scores['rouge1'].fmeasure\n",
        "        total_rouge2 += rouge_scores['rouge2'].fmeasure\n",
        "        total_rougeL += rouge_scores['rougeL'].fmeasure\n",
        "        \n",
        "        # METEOR (requires tokenized input)\n",
        "        try:\n",
        "            ref_tokens = nltk.word_tokenize(ref)\n",
        "            cand_tokens = nltk.word_tokenize(cand)\n",
        "            total_meteor += meteor_score([ref_tokens], cand_tokens)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not compute METEOR for one sample. Error: {e}\")\n",
        "\n",
        "    if num_samples == 0:\n",
        "        return {\"avg_rouge1\": 0, \"avg_rouge2\": 0, \"avg_rougeL\": 0, \"avg_meteor\": 0}\n",
        "\n",
        "    return {\n",
        "        \"avg_rouge1\": total_rouge1 / num_samples,\n",
        "        \"avg_rouge2\": total_rouge2 / num_samples,\n",
        "        \"avg_rougeL\": total_rougeL / num_samples,\n",
        "        \"avg_meteor\": total_meteor / num_samples\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "main-cell"
      },
      "source": [
        "## **Step 3**: Load Activations, Compute Vectors, and Run Test\n",
        "\n",
        "This is the main driver cell. It loads your saved `activations.npz`, calculates the PCA vector, and generates a steered response for the **second email** in your spreadsheet (index 1), showing both the \"before\" (redacted) and \"after\" (re-hydrated) results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "main-code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running inference: model=meta-llama/Llama-2-7b-hf, layer=-15\n",
            "Successfully loaded activations from 'activations.npz'\n",
            "Computing PCA style vector...\n",
            "PCA style vector computed.\n",
            "Loaded 20 rows from 'generated_email_responses_modified (2).xlsx' for testing.\n",
            "Loading Llama 2 model... (This may take a few minutes)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c963a46af99e407e8ec75e0b75a6f4a7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ModelSteeringWrapper] Model loaded. Path: ['model', 'layers'], Layers: 32\n",
            "Steering hook applied to layer -15 with multiplier 3.0.\n",
            "\n",
            "Running generation for 20 prompts...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating Responses: 100%|██████████| 20/20 [1:28:38<00:00, 265.92s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Steering hook removed.\n",
            "\n",
            "==================================================\n",
            "Calculating Evaluation Scores (ROUGE & METEOR)\n",
            "==================================================\n",
            "Average ROUGE-1 (F1): 0.3239\n",
            "Average ROUGE-2 (F1): 0.1361\n",
            "Average ROUGE-L (F1): 0.2261\n",
            "Average METEOR:     0.2418\n",
            "\n",
            "==================================================\n",
            "\n",
            "--- Example Comparisons (First 3) ---\n",
            "\n",
            "[REFERENCE 1]:\n",
            "Subject: You're Invited! Webinar: PM in a Startup Setting\n",
            "\n",
            "Hi everyone,\n",
            "\n",
            "We'd be delighted if you could join us for a special webinar hosted by Product School Bangalore!\n",
            "\n",
            "We'll be diving into \"PM in a Startup Setting\" with a fantastic speaker, the Alexa Product Lead. It's a great chance to gain insights into technology and business.\n",
            "\n",
            "Date: April 20, 2020\n",
            "Time: [Insert Time Here]\n",
            "Venue: San Francisco (Online - Link to follow!)\n",
            "\n",
            "We're really looking forward to seeing you there!\n",
            "\n",
            "Warmly,\n",
            "\n",
            "The Product School Bangalore Team\n",
            "\n",
            "[GENERATED 1]:\n",
            "Dear <RECIPIENT>,\n",
            "\n",
            "We would like to invite you to Webinar - PM in a Startup Setting_x000D_ on April 20, 2020, which starts at 6 PM at San Francisco.\n",
            "\n",
            "Product School Bangalore\n",
            "--------------------\n",
            "\n",
            "[REFERENCE 2]:\n",
            "Subject: You're Invited! Live SouJava: Microservice Patterns in San-Francisco!\n",
            "\n",
            "Hi there,\n",
            "\n",
            "We'd be delighted if you could join us for a special Product School Bangalore event!\n",
            "\n",
            "Live SouJava: Microservice Patterns - Implemented by Eclipse Microprofile\n",
            "\n",
            "It's happening on April 20th at 7:00 AM in San-Francisco. We think you'll find this blend of technology and business insights super valuable.\n",
            "\n",
            "Hope to see you there!\n",
            "\n",
            "Warmly,\n",
            "\n",
            "The Product School Bangalore Team\n",
            "\n",
            "[GENERATED 2]:\n",
            "Dear <SENDER>,\n",
            "\n",
            "I am happy to invite you to the LiveSouJava - Microservice Patterns - Implemented by Eclipse Microprofile. scheduled for April 20, 2020, at 7:00 AM in the San Francisco. The event is hosted and sent by Product School Bangalore.\n",
            "\n",
            "If you have any questions, please contact Product School Bangalore or <HOST2>.\n",
            "\n",
            "Kind regards,\n",
            "\n",
            "<SENDER>\n",
            "--------------------\n",
            "\n",
            "[REFERENCE 3]:\n",
            "Subject: You're Invited! enterprise:CODE 2020 in San Francisco!\n",
            "\n",
            "Hi everyone,\n",
            "\n",
            "We'd be thrilled if you could join us for enterprise:CODE 2020! It's going to be a fantastic event focused on technology and business, hosted by Product School Bangalore.\n",
            "\n",
            "Come connect with us in San Francisco on April 20th, starting at 7:30 AM. We can't wait to see you there!\n",
            "\n",
            "Best,\n",
            "\n",
            "The Product School Bangalore Team\n",
            "\n",
            "[GENERATED 3]:\n",
            "Dear <GUEST>,\n",
            "\n",
            "I'd like to invite you to enterprise:CODE 2020, a technology and business event on April 20, 2020, at 07:30 in San Diago_x000D_.\n",
            "\n",
            "Product School Bangalore\n",
            "\n",
            "<BODY>\n",
            "--------------------\n",
            "\n",
            "Test finished.\n"
          ]
        }
      ],
      "source": [
        "def run_inference_test(model_name: str, layer_index: int, xlsx_path: str, activations_path: str):\n",
        "    \n",
        "    # --- 1. Load Activations and Compute Vectors ---\n",
        "    try:\n",
        "        data = np.load(activations_path)\n",
        "        pos_arr = data['pos_acts']\n",
        "        neg_arr = data['neg_acts']\n",
        "        print(f\"Successfully loaded activations from '{activations_path}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading '{activations_path}'. Please run the activation extraction script first.\")\n",
        "        print(f\"Error details: {e}\")\n",
        "        return\n",
        "\n",
        "    print(\"Computing PCA style vector...\")\n",
        "    pca_style_vector = compute_pca_vector(pos_arr, neg_arr)\n",
        "    print(\"PCA style vector computed.\")\n",
        "\n",
        "    # --- 2. Load the first 20 Rows from Excel --- \n",
        "    try:\n",
        "        df = pd.read_excel(xlsx_path, nrows=20) \n",
        "        num_rows = len(df)\n",
        "        if num_rows == 0:\n",
        "            print(f\"Error: Your Excel file '{xlsx_path}' is empty.\")\n",
        "            return\n",
        "        print(f\"Loaded {num_rows} rows from '{xlsx_path}' for testing.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading Excel file '{xlsx_path}': {e}\")\n",
        "        return\n",
        "\n",
        "    # --- 3. Define Columns & Check ---\n",
        "    STYLED_COL = 'response_styled' # The *original* styled email (Reference)\n",
        "    NEUTRAL_COL = 'response_Neutral' # Used for subject line\n",
        "    FACT_COLS = ['extracted_DATE', 'extracted_TIME', 'extracted_VENUE', 'extracted_HOST', 'extracted_EVENT']\n",
        "    \n",
        "    required_cols = [STYLED_COL, NEUTRAL_COL] + FACT_COLS\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        print(f\"Error: Your Excel file is missing required columns for testing.\")\n",
        "        print(f\"Script needs: {required_cols}\")\n",
        "        print(f\"Found: {df.columns.to_list()}\")\n",
        "        return\n",
        "        \n",
        "    # --- 4. Load Model (Once) --- \n",
        "    print(\"Loading Llama 2 model... (This may take a few minutes)\")\n",
        "    ae = ModelSteeringWrapper(model_name)\n",
        "\n",
        "    # --- 5. Setup Steering Hook (Once) ---\n",
        "    MULTIPLIER = 3.0\n",
        "    hook = SteeringHook(ae.model, ae._layers_attr_path, layer_index, pca_style_vector, MULTIPLIER)\n",
        "    print(f\"Steering hook applied to layer {layer_index} with multiplier {MULTIPLIER}.\")\n",
        "\n",
        "    # --- 6. Initialize Lists and Replacer ---\n",
        "    all_references = []\n",
        "    all_candidates = []\n",
        "    replacer = PlaceholderReplacer()\n",
        "    \n",
        "    print(f\"\\nRunning generation for {num_rows} prompts...\")\n",
        "\n",
        "    try:\n",
        "        # --- 7. Loop, Generate, and Re-hydrate ---\n",
        "        for index, row in tqdm(df.iterrows(), total=num_rows, desc=\"Generating Responses\"):\n",
        "            \n",
        "            # a. Get ideal response (reference)\n",
        "            ideal_response = str(row.get(STYLED_COL, \"\"))\n",
        "            all_references.append(ideal_response)\n",
        "            \n",
        "            # b. Get facts for re-hydration\n",
        "            real_facts_dict = replacer.build_entity_dict_from_row(row, FACT_COLS)\n",
        "            \n",
        "            # c. Create the defactualized prompt\n",
        "            test_query = f\"Draft an email invitation for the <EVENT>, scheduled for <DATE>, at <TIME> in the <VENUE>. The event is hosted and sent by <HOST>.\"\n",
        "            neutral_email_text = str(row.get(NEUTRAL_COL, \"\"))\n",
        "            subject_line = \"Subject: <SUBJECT>\" # Default\n",
        "            match = re.search(r'Subject:\\\\s*(<[^>]+>.*)', neutral_email_text, re.IGNORECASE)\n",
        "            if match:\n",
        "                subject_line = match.group(0).strip()\n",
        "            prompt = f\"{test_query}\\n\\n{subject_line}\\n\\n\"\n",
        "\n",
        "            # d. Generate steered response\n",
        "            redacted_output = ae.generate(prompt, temperature=0.7, do_sample=True, top_p=0.9, max_new_tokens=250) # Increased token limit\n",
        "            \n",
        "            # e. Re-hydrate\n",
        "            final_output, _ = replacer.replace_placeholders(redacted_output, real_facts_dict)\n",
        "            all_candidates.append(final_output)\n",
        "\n",
        "    finally:\n",
        "        # --- 8. Remove Hook (Once) ---\n",
        "        hook.remove()\n",
        "        print(\"Steering hook removed.\")\n",
        "\n",
        "    # --- 9. Calculate and Print Scores ---\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Calculating Evaluation Scores (ROUGE & METEOR)\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    scores = calculate_scores(all_references, all_candidates)\n",
        "    \n",
        "    print(f\"Average ROUGE-1 (F1): {scores.get('avg_rouge1', 0):.4f}\")\n",
        "    print(f\"Average ROUGE-2 (F1): {scores.get('avg_rouge2', 0):.4f}\")\n",
        "    print(f\"Average ROUGE-L (F1): {scores.get('avg_rougeL', 0):.4f}\")\n",
        "    print(f\"Average METEOR:     {scores.get('avg_meteor', 0):.4f}\")\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    \n",
        "    # Optional: Print out the first 3 comparisons for a manual check\n",
        "    print(\"\\n--- Example Comparisons (First 3) ---\")\n",
        "    for i in range(min(3, num_rows)):\n",
        "        print(f\"\\n[REFERENCE {i+1}]:\\n{all_references[i]}\")\n",
        "        print(f\"\\n[GENERATED {i+1}]:\\n{all_candidates[i]}\")\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "    # --- Main Execution Block ---\n",
        "if __name__ == \"__main__\":\n",
        "    import sys\n",
        "    \n",
        "    if 'ipykernel' in sys.modules: sys.argv = sys.argv[:1]\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model\", type=str, default=\"meta-llama/Llama-2-7b-hf\")\n",
        "    parser.add_argument(\"--layer\", type=int, default=-15)\n",
        "    parser.add_argument(\"--xlsx_file\", type=str, default=\"generated_email_responses_modified (2).xlsx\")\n",
        "    parser.add_argument(\"--activations_file\", type=str, default=\"activations.npz\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(f\"Running inference: model={args.model}, layer={args.layer}\")\n",
        "    run_inference_test(args.model, args.layer, args.xlsx_file, args.activations_file)\n",
        "    print(\"\\nTest finished.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
