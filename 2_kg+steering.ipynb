{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install-cell"
      },
      "source": [
        "## **Step 0**: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "install-code"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch scikit-learn accelerate tqdm pandas openpyxl numpy -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "login-cell"
      },
      "source": [
        "## **Step 1**: Log in to Hugging Face\n",
        "\n",
        "Run this cell once. If you are running locally and have already used `huggingface-cli login` in your terminal, you can skip this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "login-code"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fafc38acf880432bbff53df73219bd9d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "try:\n",
        "    from huggingface_hub import notebook_login\n",
        "    notebook_login()\n",
        "except ImportError:\n",
        "    print(\"huggingface_hub not found. Please log in using 'huggingface-cli login' in your terminal.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "helper-classes-cell"
      },
      "source": [
        "## **Step 2**: Helper Classes & Functions\n",
        "\n",
        "This cell contains all helper classes:\n",
        "1.  **`ModelSteeringWrapper`**: For generation.\n",
        "2.  **`PlaceholderReplacer`**: Your code for re-hydrating text.\n",
        "3.  **`SteeringHook`**: For applying vectors.\n",
        "4.  **`compute_...` functions**: For building vectors from loaded data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "helper-code"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import argparse\n",
        "import re\n",
        "import json\n",
        "import ast\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "# --- 1. Lightweight Model Wrapper (for Generation) ---\n",
        "class ModelSteeringWrapper:\n",
        "    def __init__(self, model_name: str):\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        self.device = self.model.device\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self._layers_attr_path = self._find_layer_attr_path()\n",
        "        self.num_layers = len(self._get_layers_list())\n",
        "        print(f\"[ModelSteeringWrapper] Model loaded. Path: {self._layers_attr_path}, Layers: {self.num_layers}\")\n",
        "\n",
        "    def _find_layer_attr_path(self):\n",
        "        candidates = [[\"model\", \"layers\"], [\"transformer\", \"h\"], [\"model\", \"decoder\", \"layers\"]]\n",
        "        for path in candidates:\n",
        "            cur = self.model\n",
        "            valid = True\n",
        "            for p in path:\n",
        "                if hasattr(cur, p): cur = getattr(cur, p)\n",
        "                else: valid = False; break\n",
        "            if valid and isinstance(cur, (list, nn.ModuleList)): return path\n",
        "        raise AttributeError(\"Could not find transformer layer list in model.\")\n",
        "\n",
        "    def _get_layers_list(self):\n",
        "        cur = self.model\n",
        "        for p in self._layers_attr_path: cur = getattr(cur, p)\n",
        "        return list(cur)\n",
        "\n",
        "    def generate(self, prompt: str, max_new_tokens: int = 150, **kwargs) -> str:\n",
        "        tok = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "        input_token_len = tok.input_ids.shape[1]\n",
        "        out = self.model.generate(**tok, max_new_tokens=max_new_tokens, pad_token_id=self.tokenizer.pad_token_id, **kwargs)\n",
        "        full_tokens = out[0]\n",
        "        new_tokens = full_tokens[input_token_len:]\n",
        "        generated_text = self.tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "        return generated_text.strip()\n",
        "\n",
        "# --- 2. Your PlaceholderReplacer Class (for Re-hydration) ---\n",
        "class PlaceholderReplacer:\n",
        "    \"\"\"Replace placeholders with actual entity values from extracted columns\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.entity_types = ['EVENT', 'DATE', 'TIME', 'VENUE', 'HOST']\n",
        "    \n",
        "    def parse_entity_list(self, entity_str):\n",
        "        \"\"\"Parse string representation of list back to actual list\"\"\"\n",
        "        if pd.isna(entity_str) or entity_str == '[]' or entity_str == '':\n",
        "            return []\n",
        "        \n",
        "        try:\n",
        "            # Try to evaluate as Python literal\n",
        "            return ast.literal_eval(entity_str)\n",
        "        except:\n",
        "            # If that fails, return empty list\n",
        "            return []\n",
        "            \n",
        "    def build_entity_dict_from_row(self, row, fact_cols):\n",
        "        \"\"\"Helper to create the entity dict from a DataFrame row\"\"\"\n",
        "        entities_dict = {}\n",
        "        for entity_type in self.entity_types:\n",
        "            column_name = f'extracted_{entity_type}'\n",
        "            if column_name in fact_cols and column_name in row:\n",
        "                entity_str = row[column_name]\n",
        "                entities_dict[entity_type] = self.parse_entity_list(entity_str)\n",
        "        return entities_dict\n",
        "    \n",
        "    def replace_placeholders(self, text, entities_dict):\n",
        "        \"\"\"Replace all placeholders in text with actual entity values\"\"\"\n",
        "        \n",
        "        if not text or pd.isna(text):\n",
        "            return text, {}\n",
        "        \n",
        "        replaced_text = str(text)\n",
        "        replacement_log = {}\n",
        "        \n",
        "        # Sort entities by length of first fact (longest first) to avoid partial matches\n",
        "        sorted_entity_types = sorted(\n",
        "            self.entity_types,\n",
        "            key=lambda et: len(str(entities_dict.get(et, [''])[0])) if entities_dict.get(et) else 0,\n",
        "            reverse=True\n",
        "        )\n",
        "\n",
        "        for entity_type in sorted_entity_types:\n",
        "            entity_list = entities_dict.get(entity_type, [])\n",
        "            \n",
        "            if not entity_list:\n",
        "                continue\n",
        "            \n",
        "            placeholder = f'<{entity_type}>'\n",
        "            # Use regex for case-insensitive placeholder matching\n",
        "            placeholder_pattern = re.compile(re.escape(placeholder), re.IGNORECASE)\n",
        "            \n",
        "            # Find all matches\n",
        "            matches = list(placeholder_pattern.finditer(replaced_text))\n",
        "            placeholder_count = len(matches)\n",
        "            \n",
        "            if placeholder_count == 0:\n",
        "                continue\n",
        "            \n",
        "            replacements_made = []\n",
        "            # We reverse the matches to replace from the end first to not mess up indices\n",
        "            for i, match in enumerate(reversed(matches)):\n",
        "                # Find which entity to use\n",
        "                entity_idx = i % len(entity_list)\n",
        "                replacement_value = str(entity_list[entity_idx])\n",
        "                \n",
        "                # Replace this specific match\n",
        "                start, end = match.span()\n",
        "                replaced_text = replaced_text[:start] + replacement_value + replaced_text[end:]\n",
        "                replacements_made.append(f\"{match.group(0)} → {replacement_value}\")\n",
        "            \n",
        "            replacement_log[entity_type] = list(reversed(replacements_made))\n",
        "            \n",
        "        return replaced_text, replacement_log\n",
        "\n",
        "# --- 3. Style Vector Extraction Methods ---\n",
        "def compute_mean_difference(pos: np.ndarray, neg: np.ndarray) -> np.ndarray:\n",
        "    diff = (pos - neg).mean(axis=0)\n",
        "    return diff / (np.linalg.norm(diff) + 1e-12)\n",
        "\n",
        "def compute_logistic_regression(pos: np.ndarray, neg: np.ndarray) -> np.ndarray:\n",
        "    X = np.vstack([pos, neg])\n",
        "    y = np.concatenate([np.ones(len(pos)), np.zeros(len(neg))])\n",
        "    clf = LogisticRegression(max_iter=1000).fit(X, y)\n",
        "    w = clf.coef_.reshape(-1)\n",
        "    return w / (np.linalg.norm(w) + 1e-12)\n",
        "\n",
        "def compute_pca_vector(pos: np.ndarray, neg: np.ndarray) -> np.ndarray:\n",
        "    diffs = pos - neg\n",
        "    pca = PCA(n_components=1).fit(np.vstack([diffs, -diffs]))\n",
        "    vec = pca.components_[0]\n",
        "    return vec / (np.linalg.norm(vec) + 1e-12)\n",
        "\n",
        "# --- 4. Steering Hook Class ---\n",
        "class SteeringHook:\n",
        "    def __init__(self, model, layer_path, layer_idx, style_vector, multiplier):\n",
        "        self.model, self.layer_path, self.layer_idx = model, layer_path, layer_idx\n",
        "        self.style_vector_cpu = torch.from_numpy(style_vector).float() * multiplier\n",
        "        self.handle = None\n",
        "        self._register_hook()\n",
        "\n",
        "    def _get_layer_module(self):\n",
        "        cur = self.model\n",
        "        for p in self.layer_path: cur = getattr(cur, p)\n",
        "        idx = self.layer_idx if self.layer_idx >= 0 else len(cur) + self.layer_idx\n",
        "        return cur[idx]\n",
        "\n",
        "    def _hook(self, module, input, output):\n",
        "        tensor_output = output[0] if isinstance(output, tuple) else output\n",
        "        add_vec = self.style_vector_cpu.to(tensor_output.device, dtype=tensor_output.dtype)\n",
        "        modified_tensor = tensor_output + add_vec.view(1, 1, -1)\n",
        "        return (modified_tensor,) + output[1:] if isinstance(output, tuple) else modified_tensor\n",
        "\n",
        "    def _register_hook(self):\n",
        "        self.handle = self._get_layer_module().register_forward_hook(self._hook)\n",
        "\n",
        "    def remove(self):\n",
        "        if self.handle: self.handle.remove()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "main-cell"
      },
      "source": [
        "## **Step 3**: Load Activations, Compute Vectors, and Run Test\n",
        "\n",
        "This is the main driver cell. It loads your saved `activations.npz`, calculates the PCA vector, and generates a steered response for the **second email** in your spreadsheet (index 1), showing both the \"before\" (redacted) and \"after\" (re-hydrated) results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "main-code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running inference: model=meta-llama/Llama-2-7b-hf, layer=-15\n",
            "Successfully loaded activations from 'activations.npz'\n",
            "Computing PCA style vector...\n",
            "PCA style vector computed.\n",
            "Loaded facts for re-hydration: {'EVENT': ['LiveSouJava - Microservice Patterns - Implemented by Eclipse Microprofile.'], 'DATE': ['April 20, 2020'], 'TIME': ['7:00 AM'], 'VENUE': ['San Francisco'], 'HOST': ['Product School Bangalore']}\n",
            "Loading Llama 2 model... (This may take a few minutes)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c9899670c5049d2b5e97de64a493f07",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ModelSteeringWrapper] Model loaded. Path: ['model', 'layers'], Layers: 32\n",
            "\n",
            "==================================================\n",
            "Steering Demonstration (PCA Only, Multiplier 3.0)\n",
            "==================================================\n",
            "Test Query (with placeholders):\n",
            "Draft an email invitation for the <EVENT>, scheduled for <DATE>, at <TIME> in the <VENUE>. The event is hosted and sent by <HOST>.\n",
            "\n",
            "Subject: <SUBJECT>\n",
            "\n",
            "\n",
            "\n",
            "--- Ideal Styled Response (from file) ---\n",
            "Subject: You're Invited! Live SouJava: Microservice Patterns in San-Francisco!\n",
            "\n",
            "Hi there,\n",
            "\n",
            "We'd be delighted if you could join us for a special Product School Bangalore event!\n",
            "\n",
            "Live SouJava: Microservice Patterns - Implemented by Eclipse Microprofile\n",
            "\n",
            "It's happening on April 20th at 7:00 AM in San-Francisco. We think you'll find this blend of technology and business insights super valuable.\n",
            "\n",
            "Hope to see you there!\n",
            "\n",
            "Warmly,\n",
            "\n",
            "The Product School Bangalore Team\n",
            "\n",
            "--- Steered Generated Response (Method: PCA) ---\n",
            "\n",
            "[RAW OUTPUT (with placeholders)]:\n",
            "Dear <ADDRESS>,\n",
            "\n",
            "I'm happy to announce that <HOST> is hosting <EVENT>.\n",
            "\n",
            "The event will take place at <VENUE> on <DATE> at <TIME>.\n",
            "\n",
            "If you would like to attend, please RSVP to <HOST> at <EMAIL> or by replying to this email.\n",
            "\n",
            "Thanks!\n",
            "\n",
            "<HOST>\n",
            "\n",
            "[FINAL OUTPUT (re-hydrated)]:\n",
            "Dear <ADDRESS>,\n",
            "\n",
            "I'm happy to announce that Product School Bangalore is hosting LiveSouJava - Microservice Patterns - Implemented by Eclipse Microprofile..\n",
            "\n",
            "The event will take place at San Francisco on April 20, 2020 at 7:00 AM.\n",
            "\n",
            "If you would like to attend, please RSVP to Product School Bangalore at <EMAIL> or by replying to this email.\n",
            "\n",
            "Thanks!\n",
            "\n",
            "Product School Bangalore\n",
            "\n",
            "[Re-hydration Log]:\n",
            "{\n",
            "  \"EVENT\": [\n",
            "    \"<EVENT> \\u2192 LiveSouJava - Microservice Patterns - Implemented by Eclipse Microprofile.\"\n",
            "  ],\n",
            "  \"HOST\": [\n",
            "    \"<HOST> \\u2192 Product School Bangalore\",\n",
            "    \"<HOST> \\u2192 Product School Bangalore\",\n",
            "    \"<HOST> \\u2192 Product School Bangalore\"\n",
            "  ],\n",
            "  \"DATE\": [\n",
            "    \"<DATE> \\u2192 April 20, 2020\"\n",
            "  ],\n",
            "  \"VENUE\": [\n",
            "    \"<VENUE> \\u2192 San Francisco\"\n",
            "  ],\n",
            "  \"TIME\": [\n",
            "    \"<TIME> \\u2192 7:00 AM\"\n",
            "  ]\n",
            "}\n",
            "\n",
            "Test finished.\n"
          ]
        }
      ],
      "source": [
        "def run_inference_test(model_name: str, layer_index: int, xlsx_path: str, activations_path: str):\n",
        "    \n",
        "    # --- 1. Load Activations and Compute Vectors ---\n",
        "    try:\n",
        "        data = np.load(activations_path)\n",
        "        pos_arr = data['pos_acts']\n",
        "        neg_arr = data['neg_acts']\n",
        "        print(f\"Successfully loaded activations from '{activations_path}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading '{activations_path}'. Please run the activation extraction script first.\")\n",
        "        print(f\"Error details: {e}\")\n",
        "        return\n",
        "\n",
        "    print(\"Computing PCA style vector...\")\n",
        "    # --- MODIFICATION: Only compute PCA as requested ---\n",
        "    pca_style_vector = compute_pca_vector(pos_arr, neg_arr)\n",
        "    print(\"PCA style vector computed.\")\n",
        "\n",
        "    # --- 2. Load the Second Row (index 1) from Excel for the Test --- \n",
        "    try:\n",
        "        df = pd.read_excel(xlsx_path, nrows=2) \n",
        "        if len(df) < 2:\n",
        "            print(f\"Error: Your Excel file '{xlsx_path}' has fewer than 2 rows. Cannot test on the second row.\")\n",
        "            return\n",
        "        test_row = df.iloc[1] # Select the second row (index 1)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading Excel file '{xlsx_path}': {e}\")\n",
        "        return\n",
        "\n",
        "    # --- 3. Get Real Facts and Ideal Response from the Test Row ---\n",
        "    STYLED_COL = 'response_styled' # The *original* styled email, for comparison\n",
        "    MODIFIED_COL = 'response_Modified' # The *defactualized* styled email\n",
        "    # These are the *real* facts we will use for re-hydration.\n",
        "    FACT_COLS = ['extracted_DATE', 'extracted_TIME', 'extracted_VENUE', 'extracted_HOST', 'extracted_EVENT']\n",
        "    \n",
        "    # Check if all required columns are present in the loaded dataframe\n",
        "    if STYLED_COL not in df.columns or MODIFIED_COL not in df.columns or not all(col in df.columns for col in FACT_COLS):\n",
        "        print(f\"Error: Your Excel file is missing required columns for testing.\")\n",
        "        print(f\"Script needs: {STYLED_COL}, {MODIFIED_COL}, and all {FACT_COLS}\")\n",
        "        print(f\"Found: {df.columns.to_list()}\")\n",
        "        return\n",
        "        \n",
        "    ideal_response = str(test_row.get(STYLED_COL))\n",
        "    \n",
        "    # Instantiate the replacer and build the dictionary of real facts\n",
        "    replacer = PlaceholderReplacer()\n",
        "    real_facts_dict = replacer.build_entity_dict_from_row(test_row, FACT_COLS)\n",
        "    \n",
        "    print(f\"Loaded facts for re-hydration: {real_facts_dict}\")\n",
        "\n",
        "    # --- 4. Create the Defactualized Prompt --- \n",
        "    # This prompt must use placeholders, as the model was trained on them.\n",
        "    test_query = f\"Draft an email invitation for the <EVENT>, scheduled for <DATE>, at <TIME> in the <VENUE>. The event is hosted and sent by <HOST>.\"\n",
        "    \n",
        "    # Get the defactualized subject from the test row\n",
        "    neutral_email_text = str(test_row.get('response_Neutral'))\n",
        "    subject_line = \"Subject: <SUBJECT>\" # Default\n",
        "    match = re.search(r'Subject:\\s*(<[^>]+>.*)', neutral_email_text, re.IGNORECASE)\n",
        "    if match:\n",
        "        subject_line = match.group(0).strip()\n",
        "    \n",
        "    prompt = f\"{test_query}\\n\\n{subject_line}\\n\\n\"\n",
        "\n",
        "    # --- 5. Load Model --- \n",
        "    print(\"Loading Llama 2 model... (This may take a few minutes)\")\n",
        "    ae = ModelSteeringWrapper(model_name)\n",
        "\n",
        "    # --- 6. Run Steering Demonstration (PCA Only @ 3.0) --- \n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Steering Demonstration (PCA Only, Multiplier 3.0)\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Test Query (with placeholders):\\n{prompt}\")\n",
        "    print(\"\\n--- Ideal Styled Response (from file) ---\")\n",
        "    print(ideal_response)\n",
        "\n",
        "    MULTIPLIER = 3.0\n",
        "    method = \"pca\"\n",
        "    style_vec = pca_style_vector\n",
        "\n",
        "    print(f\"\\n--- Steered Generated Response (Method: {method.upper()}) ---\")\n",
        "    \n",
        "    hook = SteeringHook(ae.model, ae._layers_attr_path, layer_index, style_vec, MULTIPLIER)\n",
        "    try:\n",
        "        # 1. Generate the response with placeholders\n",
        "        redacted_output = ae.generate(prompt, temperature=0.7, do_sample=True, top_p=0.9)\n",
        "        print(f\"\\n[RAW OUTPUT (with placeholders)]:\\n{redacted_output}\")\n",
        "        \n",
        "        # 2. Re-hydrate the response with facts\n",
        "        final_output, log = replacer.replace_placeholders(redacted_output, real_facts_dict)\n",
        "        print(f\"\\n[FINAL OUTPUT (re-hydrated)]:\\n{final_output}\")\n",
        "        print(f\"\\n[Re-hydration Log]:\\n{json.dumps(log, indent=2)}\")\n",
        "        \n",
        "    finally:\n",
        "        hook.remove()\n",
        "\n",
        "# --- Main Execution Block ---\n",
        "if __name__ == \"__main__\":\n",
        "    import sys\n",
        "    \n",
        "    if 'ipykernel' in sys.modules: sys.argv = sys.argv[:1]\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model\", type=str, default=\"meta-llama/Llama-2-7b-hf\")\n",
        "    parser.add_argument(\"--layer\", type=int, default=-15)\n",
        "    parser.add_argument(\"--xlsx_file\", type=str, default=\"generated_email_responses_modified (2).xlsx\")\n",
        "    parser.add_argument(\"--activations_file\", type=str, default=\"activations.npz\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(f\"Running inference: model={args.model}, layer={args.layer}\")\n",
        "    run_inference_test(args.model, args.layer, args.xlsx_file, args.activations_file)\n",
        "    print(\"\\nTest finished.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
