{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-login"
      },
      "source": [
        "## **Step 0**: Log in to Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0096b0178b0a4c0182869de685030907",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-install"
      },
      "source": [
        "## **Step 1**: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers torch scikit-learn accelerate tqdm pandas openpyxl rouge-score nltk matplotlib -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-imports"
      },
      "source": [
        "## **Step 2**: Imports and NLTK Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "from typing import List, Dict, Tuple, Callable, Optional\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import argparse\n",
        "import nltk\n",
        "\n",
        "# Download NLTK data needed for METEOR\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-extractor"
      },
      "source": [
        "## **Step 3**: Model Wrapper (`ActivationExtractor`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ActivationExtractor:\n",
        "    def __init__(self, model_name: str):\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        self.device = self.model.device\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        self._layers_attr_path = self._find_layer_attr_path()\n",
        "        self.num_layers = len(self._get_layers_list())\n",
        "        print(f\"[ActivationExtractor] Model loaded. Path: {self._layers_attr_path}, Layers: {self.num_layers}\")\n",
        "\n",
        "    def _find_layer_attr_path(self):\n",
        "        candidates = [[\"model\", \"layers\"], [\"transformer\", \"h\"], [\"model\", \"decoder\", \"layers\"]]\n",
        "        for path in candidates:\n",
        "            cur = self.model\n",
        "            valid = True\n",
        "            for p in path:\n",
        "                if hasattr(cur, p): cur = getattr(cur, p)\n",
        "                else: valid = False; break\n",
        "            if valid and isinstance(cur, (list, nn.ModuleList)): return path\n",
        "        raise AttributeError(\"Could not find transformer layer list in model.\")\n",
        "\n",
        "    def _get_layers_list(self):\n",
        "        cur = self.model\n",
        "        for p in self._layers_attr_path: cur = getattr(cur, p)\n",
        "        return list(cur)\n",
        "\n",
        "    def _resolve_layer_idx(self, idx: int):\n",
        "        L = self.num_layers\n",
        "        if idx < 0: idx = L + idx\n",
        "        assert 0 <= idx < L, f\"layer_index {idx} out of range\"\n",
        "        return idx\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def get_activation_for_pair(self, input_text: str, output_text: str, layer_index: int) -> np.ndarray:\n",
        "        idx = self._resolve_layer_idx(layer_index)\n",
        "        concat = f\"{input_text.strip()} {output_text.strip()}\"\n",
        "        tok = self.tokenizer(concat, return_tensors=\"pt\").to(self.model.device)\n",
        "        outputs = self.model(**tok, output_hidden_states=True, return_dict=True)\n",
        "        hs = outputs.hidden_states[idx + 1]\n",
        "        return hs[0, -1, :].detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-data-load"
      },
      "source": [
        "## **Step 4**: Data Loading Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data_for_training_and_testing(file_path: str):\n",
        "    try:\n",
        "        df = pd.read_excel(file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while reading the Excel file: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    NEUTRAL_COL = 'response_Neutral'\n",
        "    STYLED_COL = 'response_styled'\n",
        "    \n",
        "    required_cols = ['date', 'Time', 'Venue', 'OccasionType', 'Host', 'Event', NEUTRAL_COL, STYLED_COL]\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        print(\"Error: Missing one of the required columns.\")\n",
        "        print(f\"Script needs: {required_cols}\")\n",
        "        print(f\"Found in file: {df.columns.to_list()}\")\n",
        "        return None, None\n",
        "\n",
        "    print(f\"INFO: Loaded {len(df)} examples from the file.\")\n",
        "\n",
        "    train_examples = []\n",
        "    test_examples = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        date = row.get('date', 'N/A')\n",
        "        time_ = row.get('Time', 'N/A')\n",
        "        venue = row.get('Venue', 'N/A')\n",
        "        occasion = row.get('OccasionType', 'N/A')\n",
        "        host = row.get('Host', 'N/A')\n",
        "        event = row.get('Event', 'N/A')\n",
        "        \n",
        "        neutral_email = row.get(NEUTRAL_COL)\n",
        "        styled_email = row.get(STYLED_COL)\n",
        "\n",
        "        if pd.isna(neutral_email) or pd.isna(styled_email):\n",
        "            continue\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "Write an email inviting participants to the following event.\n",
        "Ensure the email tone matches the style instruction and stays under 100 words.\n",
        "\n",
        "Event Details:\n",
        "- Date: {date}\n",
        "- Time: {time_}\n",
        "- Venue: {venue}\n",
        "- Occasion Type: {occasion}\n",
        "- Host: {host}\n",
        "- Event: {event}\n",
        "\"\"\"\n",
        "        # Add to training set (all examples)\n",
        "        train_examples.append((prompt, styled_email, neutral_email))\n",
        "\n",
        "        # Add to test set (first 20 examples)\n",
        "        if idx < 20:\n",
        "            test_examples.append((prompt, styled_email))\n",
        "\n",
        "    print(f\"Using {len(train_examples)} for training and {len(test_examples)} for testing.\")\n",
        "    \n",
        "    train_hist = {\"user_1\": train_examples} \n",
        "    test_hist = {\"user_1\": test_examples} \n",
        "\n",
        "    return train_hist, test_hist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-run-extraction"
      },
      "source": [
        "## **Step 5**: Run Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Activation Extraction: model=meta-llama/Llama-2-7b-hf, layer=-15\n",
            "INFO: Loaded 41 examples from the file.\n",
            "Using 41 for training and 20 for testing.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "472fb9b0342b467cb07f6628d3a9f91b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ActivationExtractor] Model loaded. Path: ['model', 'layers'], Layers: 32\n",
            "\n",
            "[Pipeline] Extracting activations for 'user_1' with 41 examples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting training activations: 100%|██████████| 41/41 [56:00<00:00, 81.96s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[SUCCESS] Activations saved successfully to 'activations.npz'\n",
            "You can now run the '2_Evaluate_From_Saved.ipynb' notebook.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # This fix is for Jupyter notebooks\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        sys.argv = sys.argv[:1]\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model\", type=str, default=\"meta-llama/Llama-2-7b-hf\")\n",
        "    parser.add_argument(\"--layer\", type=int, default=-15)\n",
        "    parser.add_argument(\"--xlsx_file\", type=str, default=\"generated_email_responses (1).xlsx\")\n",
        "    parser.add_argument(\"--output_file\", type=str, default=\"activations.npz\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(f\"Running Activation Extraction: model={args.model}, layer={args.layer}\")\n",
        "    \n",
        "    # 1. Load data\n",
        "    train_hist, test_hist = load_data_for_training_and_testing(args.xlsx_file)\n",
        "    if not train_hist:\n",
        "        print(\"Halting execution due to data loading error.\")\n",
        "    else:\n",
        "        # 2. Load model\n",
        "        ae = ActivationExtractor(args.model)\n",
        "\n",
        "        # 3. Extract Activations on all 40 examples\n",
        "        user_id = \"user_1\"\n",
        "        examples = train_hist[user_id]\n",
        "        \n",
        "        print(f\"\\n[Pipeline] Extracting activations for '{user_id}' with {len(examples)} examples...\")\n",
        "        pos_acts, neg_acts = [], []\n",
        "        for (inp_prompt, user_out, neutral_out) in tqdm(examples, desc=\"Extracting training activations\"):\n",
        "            pos_acts.append(ae.get_activation_for_pair(inp_prompt, user_out, args.layer))\n",
        "            neg_acts.append(ae.get_activation_for_pair(inp_prompt, neutral_out, args.layer))\n",
        "\n",
        "        # 4. Save the activations to a file\n",
        "        pos_arr = np.vstack(pos_acts)\n",
        "        neg_arr = np.vstack(neg_acts)\n",
        "        \n",
        "        np.savez_compressed(args.output_file, pos_acts=pos_arr, neg_acts=neg_arr)\n",
        "        \n",
        "        print(f\"\\n[SUCCESS] Activations saved successfully to '{args.output_file}'\")\n",
        "        #print(\"You can now run the '2_Evaluate_From_Saved.ipynb' notebook.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
