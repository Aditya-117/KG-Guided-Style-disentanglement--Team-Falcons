{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a212a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ed645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "class EventKnowledgeGraphExtractor:\n",
    "    def __init__(self):\n",
    "        # Enhanced date patterns\n",
    "        self.date_patterns = [\n",
    "            # Full month names with optional ordinal suffixes\n",
    "            r'\\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2}(?:st|nd|rd|th)?,?\\s+\\d{4}\\b',\n",
    "            # Abbreviated month names with optional ordinal suffixes  \n",
    "            r'\\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Sept|Oct|Nov|Dec)\\.?\\s+\\d{1,2}(?:st|nd|rd|th)?,?\\s+\\d{4}\\b',\n",
    "            # Numeric formats\n",
    "            r'\\b\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}\\b',\n",
    "            r'\\b\\d{4}[-/]\\d{1,2}[-/]\\d{1,2}\\b',\n",
    "        ]\n",
    "        \n",
    "        # Enhanced time patterns\n",
    "        self.time_patterns = [\n",
    "            # Standard 12-hour format with AM/PM (with or without space)\n",
    "            r'\\b\\d{1,2}:\\d{2}\\s*(?:AM|PM|am|pm|a\\.m\\.|p\\.m\\.)\\b',\n",
    "            # Just hour with AM/PM\n",
    "            r'\\b\\d{1,2}\\s*(?:AM|PM|am|pm|a\\.m\\.|p\\.m\\.)\\b',\n",
    "            # With timezone\n",
    "            r'\\b\\d{1,2}:\\d{2}\\s*(?:AM|PM|am|pm)\\s*(?:PST|EST|IST|CST|MST|PDT|EDT|CDT|MDT)\\b',\n",
    "            # 24-hour format\n",
    "            r'\\b\\d{1,2}:\\d{2}\\b(?=\\s*(?:hours?|in|on|at|$))',\n",
    "        ]\n",
    "        \n",
    "        # Online/Virtual venue patterns\n",
    "        self.online_venue_patterns = [\n",
    "            r'\\b(ONLINE)\\b',\n",
    "            r'\\b(ZOOM)\\b',\n",
    "            r'\\b(Virtual)\\b',\n",
    "            r'\\b(Webinar)\\b',\n",
    "            r'\\b(Online\\s+Event)\\b',\n",
    "            r'\\b(Virtual\\s+Event)\\b',\n",
    "        ]\n",
    "        \n",
    "        # Location patterns (including typos)\n",
    "        self.location_patterns = [\n",
    "            r'\\b(San[\\s-]Francisco)\\b',\n",
    "            r'\\b(San[\\s-]Daiago)\\b',\n",
    "            r'\\b(Sans\\s+Francisco)\\b',\n",
    "            r'\\b(Bangalore)\\b',\n",
    "            r'\\b(Riyadh)\\b',\n",
    "            r'\\bVenue:\\s*([A-Za-z\\s,]+?)(?=\\n|$)',\n",
    "        ]\n",
    "        \n",
    "        # Host patterns\n",
    "        self.host_patterns = [\n",
    "            r'hosted by\\s+([^!.\\n]+)',\n",
    "            r'[Ww]armly,?\\s*\\n\\s*(.+?)(?:\\n|$)',\n",
    "            r'[Bb]est,?\\s*\\n\\s*(.+?)(?:\\n|$)',\n",
    "            r'Product School\\s+\\w+',\n",
    "        ]\n",
    "        \n",
    "    def extract_event_name(self, text):\n",
    "        \"\"\"Extract event name from subject line or body\"\"\"\n",
    "        # Try to get from subject line\n",
    "        subject_match = re.search(r'Subject:\\s*(?:You\\'re Invited!\\s*)?(.+?)(?:\\n|$)', text, re.IGNORECASE)\n",
    "        if subject_match:\n",
    "            event = subject_match.group(1).strip()\n",
    "            # Clean up common suffixes\n",
    "            event = re.sub(r'\\s+in\\s+San[\\s-]Francisco.*$', '', event, flags=re.IGNORECASE)\n",
    "            event = re.sub(r'\\s+with\\s+Product School.*$', '', event, flags=re.IGNORECASE)\n",
    "            event = re.sub(r'^You\\'re Invited!\\s*', '', event, flags=re.IGNORECASE)\n",
    "            return event.strip()\n",
    "        \n",
    "        # Fallback: look for quoted event names\n",
    "        quoted = re.search(r'\"([^\"]+)\"', text)\n",
    "        if quoted:\n",
    "            return quoted.group(1).strip()\n",
    "        \n",
    "        return \"Unknown Event\"\n",
    "    \n",
    "    def extract_date(self, text):\n",
    "        \"\"\"Extract date using multiple patterns\"\"\"\n",
    "        for pattern in self.date_patterns:\n",
    "            match = re.search(pattern, text, re.IGNORECASE)\n",
    "            if match:\n",
    "                return match.group(0).strip()\n",
    "        return \"Date Not Found\"\n",
    "    \n",
    "    def extract_time(self, text):\n",
    "        \"\"\"Extract time using multiple patterns\"\"\"\n",
    "        for pattern in self.time_patterns:\n",
    "            match = re.search(pattern, text, re.IGNORECASE)\n",
    "            if match:\n",
    "                return match.group(0).strip()\n",
    "        return \"Time Not Found\"\n",
    "    \n",
    "    def extract_venue(self, text):\n",
    "        \"\"\"Extract venue with priority for physical locations\"\"\"\n",
    "        # First check for online venues\n",
    "        for pattern in self.online_venue_patterns:\n",
    "            match = re.search(pattern, text, re.IGNORECASE)\n",
    "            if match:\n",
    "                online_venue = match.group(1)\n",
    "                # Still check for physical location mentioned\n",
    "                for loc_pattern in self.location_patterns:\n",
    "                    loc_match = re.search(loc_pattern, text, re.IGNORECASE)\n",
    "                    if loc_match:\n",
    "                        return f\"{loc_match.group(1)} (Online)\"\n",
    "                return online_venue\n",
    "        \n",
    "        # Check for physical locations\n",
    "        for pattern in self.location_patterns:\n",
    "            match = re.search(pattern, text, re.IGNORECASE)\n",
    "            if match:\n",
    "                return match.group(1).strip()\n",
    "        \n",
    "        return \"Venue Not Found\"\n",
    "    \n",
    "    def extract_host(self, text):\n",
    "        \"\"\"Extract host/organizer information\"\"\"\n",
    "        for pattern in self.host_patterns:\n",
    "            match = re.search(pattern, text, re.IGNORECASE)\n",
    "            if match:\n",
    "                host = match.group(1).strip()\n",
    "                # Clean up\n",
    "                host = re.sub(r'\\[.*?\\]', '', host)\n",
    "                host = re.sub(r'\\s+', ' ', host)\n",
    "                if len(host) > 3 and len(host) < 100:\n",
    "                    return host\n",
    "        \n",
    "        return \"Product School Bangalore\"  # Default host from data\n",
    "    \n",
    "    def extract_entities(self, text):\n",
    "        \"\"\"Extract all entities from text\"\"\"\n",
    "        return {\n",
    "            'event': self.extract_event_name(text),\n",
    "            'date': self.extract_date(text),\n",
    "            'time': self.extract_time(text),\n",
    "            'venue': self.extract_venue(text),\n",
    "            'host': self.extract_host(text)\n",
    "        }\n",
    "    \n",
    "    def create_knowledge_graph(self, entities, row_id):\n",
    "        \"\"\"Create a NetworkX knowledge graph for extracted entities\"\"\"\n",
    "        G = nx.DiGraph()\n",
    "        \n",
    "        # Central event node\n",
    "        event_node = f\"Event_{row_id}\"\n",
    "        G.add_node(event_node, type='event', name=entities['event'])\n",
    "        \n",
    "        # Add entity nodes and relationships\n",
    "        if entities['date'] != \"Date Not Found\":\n",
    "            date_node = f\"Date_{row_id}\"\n",
    "            G.add_node(date_node, type='date', value=entities['date'])\n",
    "            G.add_edge(event_node, date_node, relation='occurs_on')\n",
    "        \n",
    "        if entities['time'] != \"Time Not Found\":\n",
    "            time_node = f\"Time_{row_id}\"\n",
    "            G.add_node(time_node, type='time', value=entities['time'])\n",
    "            G.add_edge(event_node, time_node, relation='starts_at')\n",
    "        \n",
    "        if entities['venue'] != \"Venue Not Found\":\n",
    "            venue_node = f\"Venue_{row_id}\"\n",
    "            G.add_node(venue_node, type='venue', value=entities['venue'])\n",
    "            G.add_edge(event_node, venue_node, relation='located_at')\n",
    "        \n",
    "        if entities['host']:\n",
    "            host_node = f\"Host_{row_id}\"\n",
    "            G.add_node(host_node, type='host', value=entities['host'])\n",
    "            G.add_edge(host_node, event_node, relation='hosts')\n",
    "        \n",
    "        return G\n",
    "    \n",
    "    def visualize_graph(self, G, filename, output_dir):\n",
    "        \"\"\"Visualize and save knowledge graph\"\"\"\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        pos = nx.spring_layout(G, k=2, iterations=50)\n",
    "        \n",
    "        # Color nodes by type\n",
    "        color_map = {\n",
    "            'event': '#FF6B6B',\n",
    "            'date': '#4ECDC4',\n",
    "            'time': '#45B7D1',\n",
    "            'venue': '#FFA07A',\n",
    "            'host': '#98D8C8'\n",
    "        }\n",
    "        \n",
    "        node_colors = [color_map.get(G.nodes[node].get('type', 'event'), '#gray') \n",
    "                       for node in G.nodes()]\n",
    "        \n",
    "        # Draw graph\n",
    "        nx.draw(G, pos, node_color=node_colors, node_size=3000, \n",
    "                with_labels=False, arrows=True, edge_color='gray',\n",
    "                arrowsize=20, arrowstyle='->', width=2)\n",
    "        \n",
    "        # Add labels\n",
    "        labels = {}\n",
    "        for node in G.nodes():\n",
    "            node_data = G.nodes[node]\n",
    "            if 'name' in node_data:\n",
    "                labels[node] = node_data['name'][:30]\n",
    "            elif 'value' in node_data:\n",
    "                labels[node] = node_data['value'][:30]\n",
    "            else:\n",
    "                labels[node] = node[:15]\n",
    "        \n",
    "        nx.draw_networkx_labels(G, pos, labels, font_size=8)\n",
    "        \n",
    "        # Add edge labels\n",
    "        edge_labels = nx.get_edge_attributes(G, 'relation')\n",
    "        nx.draw_networkx_edge_labels(G, pos, edge_labels, font_size=7)\n",
    "        \n",
    "        plt.title(f\"Knowledge Graph: {filename}\", fontsize=14, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save\n",
    "        output_path = os.path.join(output_dir, f\"{filename}.png\")\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def process_excel(self, excel_path, column_name, output_dir='KG_Output'):\n",
    "        \"\"\"Main processing function\"\"\"\n",
    "        # Create output directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        graphs_dir = os.path.join(output_dir, 'graphs')\n",
    "        os.makedirs(graphs_dir, exist_ok=True)\n",
    "        \n",
    "        # Read Excel\n",
    "        print(f\"Reading Excel file: {excel_path}\")\n",
    "        df = pd.read_excel(excel_path)\n",
    "        \n",
    "        if column_name not in df.columns:\n",
    "            raise ValueError(f\"Column '{column_name}' not found in Excel file!\")\n",
    "        \n",
    "        # Storage for results\n",
    "        all_graphs = {}\n",
    "        extracted_data = []\n",
    "        \n",
    "        # Process each row\n",
    "        for idx, row in df.iterrows():\n",
    "            text = str(row[column_name])\n",
    "            \n",
    "            if pd.isna(text) or text.strip() == '':\n",
    "                continue\n",
    "            \n",
    "            print(f\"Processing row {idx + 1}/{len(df)}...\")\n",
    "            \n",
    "            # Extract entities\n",
    "            entities = self.extract_entities(text)\n",
    "            \n",
    "            # Create knowledge graph\n",
    "            G = self.create_knowledge_graph(entities, idx)\n",
    "            all_graphs[f\"event_{idx}\"] = G\n",
    "            \n",
    "            # Visualize and save graph\n",
    "            self.visualize_graph(G, f\"KG_Event_{idx}\", graphs_dir)\n",
    "            \n",
    "            # Store extracted data\n",
    "            extracted_data.append({\n",
    "                'Row_ID': idx,\n",
    "                'Extracted_Event': entities['event'],\n",
    "                'Extracted_Host': entities['host'],\n",
    "                'Extracted_Venue': entities['venue'],\n",
    "                'Extracted_Date': entities['date'],\n",
    "                'Extracted_Time': entities['time']\n",
    "            })\n",
    "        \n",
    "        # Save all graphs as pickle\n",
    "        graphs_path = os.path.join(output_dir, 'all_knowledge_graphs.pkl')\n",
    "        with open(graphs_path, 'wb') as f:\n",
    "            pickle.dump(all_graphs, f)\n",
    "        print(f\"Saved all knowledge graphs to: {graphs_path}\")\n",
    "        \n",
    "        # Create output DataFrame and Excel\n",
    "        output_df = pd.DataFrame(extracted_data)\n",
    "        output_excel_path = os.path.join(output_dir, 'extracted_KG_entities.xlsx')\n",
    "        output_df.to_excel(output_excel_path, index=False)\n",
    "        print(f\"Saved extracted entities to: {output_excel_path}\")\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EXTRACTION SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Total events processed: {len(extracted_data)}\")\n",
    "        print(f\"Knowledge graphs created: {len(all_graphs)}\")\n",
    "        print(f\"Graphs visualized: {len(extracted_data)}\")\n",
    "        print(f\"\\nOutput files:\")\n",
    "        print(f\"  - Entities Excel: {output_excel_path}\")\n",
    "        print(f\"  - Graphs Pickle: {graphs_path}\")\n",
    "        print(f\"  - Graph Images: {graphs_dir}/\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return output_df, all_graphs\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize extractor\n",
    "    extractor = EventKnowledgeGraphExtractor()\n",
    "    \n",
    "    # Configuration\n",
    "    EXCEL_FILE = \"generated_email_responses_1 user.xlsx\"\n",
    "    COLUMN_NAME = \"response_Casual_Friendly_User\"\n",
    "    OUTPUT_DIR = \"KG_Output\"\n",
    "    \n",
    "    try:\n",
    "        # Process the Excel file\n",
    "        extracted_df, knowledge_graphs = extractor.process_excel(\n",
    "            excel_path=EXCEL_FILE,\n",
    "            column_name=COLUMN_NAME,\n",
    "            output_dir=OUTPUT_DIR\n",
    "        )\n",
    "        \n",
    "        # Display sample results\n",
    "        print(\"\\nSample Extracted Data:\")\n",
    "        print(extracted_df.head(10).to_string())\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Excel file '{EXCEL_FILE}' not found!\")\n",
    "        print(\"Please ensure the file is in the same directory as this script.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
